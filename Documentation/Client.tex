\jsection{Client}
	
	One of our groups major design choices for this project was to split the logic and the network calls and handling into two different threads.  This allowed our group to abstract system calls into a different thread of execution and create a system that can react to multiple different inputs rather than block on one system call.  This also means that client logic does not stop if the tracker crashes, so if the network thread receives chunks and then the tracker crashes then it will not shut down the logic and the logic will be able to save its working set of data.  This section will explain the logic portion of the client and the network section will detail the network functionality.

	\jsubsection{How To Build and Run the Client}

	Our DartSync project team chose to split the executables for the tracker and client.  However, we still build with one Makefile.  To build the entire project, execute ``make'' from the command terminal in our top level folder, or to simply make the client you can execute ``make client'' from the command terminal also in our top level folder.  \\

	To run the client, simply call either ./client/client_app \{Tracker IP address\} \{Folder To Sync\}from the top level folder, or just call ./client_app from the /client/ folder.  The input is as follows:
	
	\begin{itemize}
		\item \{Tracker IP address\} (required) - the IP address of the tracker network that you would like to connect to.  The address is parsed to check for validity, and connected to before continuing with the logic code to make sure that the input is valid.
		\item \{Folder To Sync\} (optional) - the folder to sync, if you do not provide a command line input for this, then the program will default to the creation and use of a ``~/dart_sync'' folder.  If the provided folder is invalid, then the client exits and if the client fails to create ``~/dart_sync'' then it exits.
	\end{itemize}

	\jsubsection{Main Loop Structure}

	After checking inputs, the client starts the network thread and connects to the tracker logic. After this, the client will make requests to the tracker for the master filesystem structure and the master filetable structure. Once the client receives the master filesystem and filetable and matches its own local state to this master state by deleting any untracked files, letting the tracker know what it has that matches the master, and making requests for files it doesn't have, it will then begin monitoring the main loop.  The main loop consists of several calls that check queues until they are empty or monitor other important structures.  If there are changes in any of these calls then the client reacts accordingly:

	\begin{enumerate}
		\item recv_peer_added() - returns the client ID of a new peer that has joined the network or NULL if there is nothing in the queue and therefore there are no new clients that the network thread knows about.  New peer updates are received from the tracker and are used by the network side to create a new mapping of peer id to peer IP address for any future connections that need to be made.  We simply print an acknowledgement that the peer has dropped, it is not used unless it has a chunk in the filetable that we need.
		\item recv_peer_deleted() - returns the client ID of a peer that has dropped out of the network or NULL if there are no peers to remove.  If there is a peer to remove, then we iterate over the file table and remove them from the chunk ownership list if they have that chunk.
		\item receive_chunk_request() - If this returns 1 then a peer has requested a chunk from us.  We check to make sure that we have it (if we don't then we send a failure message) and then send them the chunk they requested.  If this call returns -1 then there are no pending chunk requests.
		\item receive_chunk() - If this returns 1 then there was a chunk waiting for us on the queue and we need to insert it into the chunkyfile and acknowledge that we do not need to rerequest. Before it gets inserted into the ChunkyFile, its ID gets checked against the list of valid IDs in FileTable. If the chunk has a valid ID, then it is inserted. Otherwise, it gets ignored. Once all outstanding chunks for a file are received, we save the file to disk.  If this returns -1 then there were no pending chunks on the queue.
		\item receive_chunk_got() - If this returns 1 then we received an acknowledgement from the tracker that a client received a chunk and it is OK to request it from them.  If this returns -1 then there were no pending chunk acquisition updates on the queue.
		\item recv_diff() - If this returns 1 then we received a filesystem diff initiated by another peer from the tracker.  We first remove any file deletions from the local disk, the file system struct, and the file table struct, and then we add any additions to the filesystem and file tables structs and write the new empty files to disk and make chunk requests for all the chunks.  If this returns -1 then there were no pending diffs on the queue.
		\item recv_master() - This should only return 1 if the connection to the tracker is reset for some reason, the tracker will resend the master filesystem and we need to match up with it. If it returns -1 then it means there was no problem and there is nothing on the queue.
		\item check_work_queue() - This function's job is to check to see if any files in the FileTable have outstanding chunks that they need to request, and if there are, then to request them. It does this by iterating through the FileTable. For each file in the FileTable, if there are chunks that need to be requested, and not too many chunks are currently in transit, then request enough chunks.
		\item CheckLocalFilesystem() - This is a void return function that will check to see if there are any local changes to the filesystem.  If there are then it will push the diff to the tracker who will distribute the change to the rest of the peers in the network.  If there are no local changes then the function will do clean up and continue with execution of the main loop.
	\end{enumerate}

	\jsubsection{Work Queue}

	This section will discuss the process of acquiring chunks of data from other clients, how chunks are requested, and who they are requested from. Whenever a client receives a diff from the tracker, it also receives the ID of the peer that originated that diff. The client then adds that diff to its own FileSystem and FileTable. In the process of adding the diff to the FileTable, any previous information regarding the files that were added gets destroyed (this becomes important later). Since the client is hearing about this set of files for the first time, it only knows one thing: that the originator of the file has all of its chunks. Using this information, it creates a chunk list for the file (a 2d array that maps the chunk number with a list of peers that have that chunk) and initializes it with the peer ID of the originator.
	
	Now, instead of requesting all of the chunks immediately which effectively means that all of the chunks would be acquired from one peer (because we only know about one peer), the client initializes a queue of chunk numbers and then shuffles the queue so that it is in a random order. This is called the Work Queue. It also initializes an empty Queue called the Outstanding Request Queue.
	
	When the function check_work_queue() gets called, the client looks at two pieces of information for each file to decide if it should request chunks of that file. First, it looks to see if there are any chunks on the Work Queue. If there are, then it looks at the Outstanding Request Queue. If the Outstanding Request Queue contains fewer than 15 items, then the client decides it is time to request more. Until one or both of these conditions is not met, the client loops and at each loop, requests a job from the FileTable. This job consists of three pieces of information: the chunk number, the destination peer's ID, and the job ID. The chunk number comes directly off of the Work Queue, and since the Work Queue is shuffled, the chunk numbers are retrieved in a random order. The destination peer's ID is chosen randomly from the list of peers that have the chosen chunk. The job ID is a unique number that will be associated with this particular request. Next, two things happen. First, the job ID gets pushed onto the Outstanding Request Queue. Second, the chunk number and job ID are packaged together and sent to the specified peer.
	
	When a peer receives a chunk request, it gets the chunk of the requested file and returns it to the specified peer. In this response, it includes the unique ID that it received.
	
	When a peer receives a chunk, it also gets the unique ID of the request. Before it inserts the chunk into the ChunkyFile, the peer checks to see if the unique ID is contained in the correct Outstanding Request Queue in the FileTable. If it is, then it goes ahead. If it is not, then it ignores the chunk. Because when a client receives a diff, it removes all information about said file from the FileTable (including the Outstanding Request Queue), this behavior ensures that if a file changes while a request is in transit, when the request gets back to the client, the client will not accept the request. Also, since the number of outstanding requests is limited, there is time for information about which chunks other peers have acquired to get back to each peer during the sharing process. Since the chunks are downloaded in a random order, as each peer requests more and more chunks, there is a higher likelihood that the peers will start downloading chunks from one another, instead of from the originator of the file. This significantly increases download speed and reduces the impact on the originating peer.

	\jsubsection{Output}

	Other than writing file system updates to disk, the only output of the client executable is print statements to the command terminal.  These statements describe an event that occurred or an action that the logic is taking to react to that event.

	\begin{enumerate}
		\item (``CLIENT MAIN: '' statements): anything beginning with ``CLIENT MAIN: '' is a reaction to some event that occured that matches the previously enumerated main loop items.  The only statement that breaks this trend is ``CLIENT MAIN: sending chunk (\%s, \%d) to peer \%d''.  This statement lets the user know that it received a request for chunk \%d of file \%s and is sending it to client \%d.  
			\begin{itemize}
				\item EX: ``CLIENT MAIN: received a diff from author \%d'' lets the user know that the client recieved a diff off the queue from peer \%d and will begin responding to it.
			\end{itemize}
		\item The other print statements describe what a function is doing to react to that such as sending a request for a chunk.  One example is 
			\begin{itemize}
				\item EX: ``RemoveFileDeletions: found deletion at: \%s'' lets the user know that the client funciton RemoveFileDeletions was called after responding to a diff and found a deletion at the local absolute path \%s.  The program then deletes this file.
			\end{itemize}
	\end{enumerate}

	
	